postgres:
  metrics:
    enabled: true

  masterAntiAffinity: {}
    # soft: true
    # mastermasterAntiaffinity:  100
    # masterslaveAntiaffinity: 50

  slaveAntiAffinity: {}
    # soft: true
    # weight: 50

  # Values in {
  # - DEBUG1..DEBUG5
  # - INFO
  # - NOTICE
  # - WARNING
  # - ERROR
  # - LOG
  # - FATAL
  # - PANIC
  # }
  debug: WARNING

  image:
    initContainer:
      pullPolicy: "IfNotPresent"
      registry: {}
      registryPort: {}
      name: "postgres"
      # Alpine images for init container to reduce
      # overal resource strain with Image size: 28MBs
      tag: "11.5-alpine"

      securityContext:
        runAsUser: 0
        privileged: true

      resources: {}
        # limits:
        #   memory: "35Mi"
        #   cpu: "150m"
        # requests:
        #   memory: "25Mi"
        #   cpu: "100m"

    mainContainer:
      pullPolicy: "IfNotPresent"
      registry: {}
      registryPort: {}
      name: "postgres"
      # Debian based image
      # Image size : 117MB
      tag: "11.5"

      securityContext:
        runAsUser: 999
        runAsGroup: 999
        privileged: false
        readOnlyRootFilesystem: false

    exporterContainer:
      pullPolicy: "IfNotPresent"
      registry: {}
      registryPort: {}
      name: "wrouesnel/postgres_exporter"
      # Debian based image
      # Image size : 5.65MB
      tag: "v0.5.1"

  master:
    extraConfigs: {}
    updateStrategy: "RollingUpdate"
    # Desired master statefulset replicas
    # Suggestion : ~= 1
    #              or multiple but requires strict separation of R/W stream
    #                 due to lack of consistency between masters
    replicas: {}
    resources: {}
      # requests:
      #   memory: "16Gi"
      #   cpu: "16"

    podDisruptionBudget: {}
      # #    Warning :  this won't be applied unless the replicas
      # #               values are >= 2
      # minAvailable: 1

    podPriorityClass:
      # If .Values.global.podPriorityClass.enabled and spec is {}
      # defaults to
      #   globalDefault: false
      #   value: 100000
      #   preemptionPolicy: PreemptLowerPriority
      spec: {}

    # Allows pods to live a certain period after termination
    terminationGracePeriodSeconds: 10

    readinessProbe:
      enabled: true
      # Worst case scenario time of failed response before container is considered ready by k8s :
      # initialDelaySeconds + ((periodSeconds + timeoutSeconds) * failureThreshold)
      # = 29s ~= .5min
      initialDelaySeconds: 5
      periodSeconds: 5
      timeoutSeconds: 3
      failureThreshold: 3

    livenessProbe:
      enabled: true
      # Worst case scenario time of failed response before container is killed by k8s :
      # initialDelaySeconds + ((periodSeconds + timeoutSeconds) * failureThreshold)
      # = 220s ~= 4min
      initialDelaySeconds: 60
      periodSeconds: 5
      timeoutSeconds: 3
      failureThreshold: 20

    # This value is injected to postgres using the env variable $PG_DATA
    # and the default value is the one recommended for postgresql imaes
    pvcMountPath: "/var/lib/postgresql/data"
    # Injecting this value ignores all other requests and accessModes input
    # If set to { }, uses the other values
    existingClaim: {}
    accessModes: "ReadWriteOnce"
    requests:
      # After populating the database with go load and performing a global scan with
      # patchlibanalyzer, the total database weights around 454M, but for actual uses:
      # - 10Gi would be enough for very small test environments
      # - 100Gi recommended for small-medium production environments
      # - 500Gi recommended for big production environments (which is our case)
      storage: "10Gi"

  slave:
    backoffDuration: 10
    resources: {}
      # requests:
      #   memory: "8Gi"
      #   cpu: "8"
      # limits:
      #   memory: "15Gi"
      #   cpu: "15"
    extraConfigs: {}


    # Warning: this won't be applied unless the replicas
    #          values are >= 2
    podDisruptionBudget: {}
    podPriorityClass:

      # If .Values.global.podPriorityClass.enabled and spec is {}
      # defaults to
      #   globalDefault: false
      #   value: 40000
      #   preemptionPolicy: PreemptLowerPriority
      spec: {}

    # Replicas will be dynamically calculated if set to {}
    # # Desired slave statefulset replicas
    # # Suggestion : ~= R * master stateful set replicas
    # #                 with R the desired replication factor
    # # Replicas will be dynamically calculated if set to {}
    # replicas: 3
    updateStrategy: "RollingUpdate"
    terminationGracePeriodSeconds: 10

    readinessProbe:
      # Worst case scenario time of failed response before container is considered ready by k8s :
      # initialDelaySeconds + ((periodSeconds + timeoutSeconds) * failureThreshold)
      # = 94s ~= 1.5min
      enabled: true
      initialDelaySeconds: 40
      periodSeconds: 15
      timeoutSeconds: 3
      failureThreshold: 3

    livenessProbe:
      enabled: false
      # Worst case scenario time of failed response before container is killed by k8s :
      # initialDelaySeconds + ((periodSeconds + timeoutSeconds) * failureThreshold)
      # = 220s ~= 4min
      initialDelaySeconds: 40
      periodSeconds: 15
      timeoutSeconds: 3
      failureThreshold: 10

    # This value is injected to postgres using the env variable $PG_DATA
    # and the default value is the one recommended for postgresql images
    pvcMountPath: "/var/lib/postgresql/data"
    accessModes: "ReadWriteOnce"
    requests:
      storage: "10Gi"


pgpool:
  name: pgpool
  # If set to false this sets CRUNCHY_DEBUG="False" as an env variable
  # else CRUNCHY_DEBUG="true" because this is a crunchy pgpool image
  debug: false
  # Suggestion : ~= master statefulset replicas + slave statefulset replicas
  # dynamically computed if set to {}
  replicas: 3

  podPriorityClass:
    # If .Values.global.podPriorityClass.enabled and spec is {}
    # defaults to
    #   globalDefault: false
    #   value: 30000
    #   preemptionPolicy: PreemptLowerPriority
    spec: {}

  # Allows for automated rolling updates
  updateStrategy: "RollingUpdate"
  podDisruptionBudget: {}
    # #    Warning :  this won't be applied unless the replicas
    # #               values are >= 2
    # # Pod disruption budget can be set dynamically or statically
    # # dynamic: sum ( masters + replication factor * master )
    # dynamic: true
    # # minAvailable: 1

  antiAffinity: {}
    # soft: true
    # # _d_ antiAffinity towards own pods
    # selfAntiAffinity: 50

  affinity: {}
    # soft: true
    # # affinity towards being close to postgres replica nodes
    # poolreplicaAffinity: 50
    # # affinity towards being close to postgres master nodes
    # # preference higher than replica nodes because write functions
    # # are only available on master nodes
    # poolmasterAffinity: 70

  loadBalanceMode: on
  failOverOnBackendError: off
  numInitChildren: 100

  maxPool: 10
  clientIdleLimit: 920
  connectionLifeTime: 500

  # Allows pods to live a certain period after termination
  terminationGracePeriodSeconds: 15
  useWatchDog: on
  watchDog:
    interval: 15

  healthCheck:
    timeout: 0
    period: 30
    maxRetries: 5
    retryDelay: 5

  image:
    pullPolicy: "IfNotPresent"
    registry: {}
    registryPort: {}
    name: "crunchydata/crunchy-pgpool"
    tag: "centos7-11.4-2.4.1"

    securityContext:
      runAsUser: 999
      runAsGroup: 999
      privileged: false
      readOnlyRootFilesystem: false

    resources: {}
      # limits:
      #   memory: "2Gi"
      #   cpu: "2000m"
      # requests:
      #   memory: "1Gi"
      #   cpu: "1000m"

    readinessProbe:
      # Worst case scenario time of failed response before container is considered ready by k8s :
      # initialDelaySeconds + ((periodSeconds + timeoutSeconds) * failureThreshold)
      # = 95s ~= 1.5min
      enabled: true
      initialDelaySeconds: 35
      periodSeconds: 10
      timeoutSeconds: 10
      failureThreshold: 3

    livenessProbe:
      # Worst case scenario time before container is killed by k8s :
      # initialDelaySeconds + ((periodSeconds + timeoutSeconds) * failureThreshold)
      # = 155s ~= 2.5min
      enabled: true
      initialDelaySeconds: 35
      periodSeconds: 30
      timeoutSeconds: 10
      failureThreshold: 3
