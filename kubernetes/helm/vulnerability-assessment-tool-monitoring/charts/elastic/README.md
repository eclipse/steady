
[![version](https://img.shields.io/badge/version-v0.1.1-brightgreen.svg)](https://shields.io/) [![githublicense](https://img.shields.io/badge/license-Apache_2.0-blue.svg)](https://shields.io/)

# Elastic: Kibana + Elasticsearch

This chart is a modified version of the stable chart (see ![source](https://github.com/helm/charts/tree/master/stable/) creates a and an cluster for visualizing and storing logs from the cluster. It also launches some jobs to set up log levels as well as ILM (Index Lifecycle Management) for the ES cluster.

## .Prerequisites
- Kubernetes 1.9 with Beta APIs enabled
- Persistent volume provisionner support in underlying infrastructure

## .Installing the chart
To install the chart with the release name `release`:
```console
$ helm install release elastic
```

The command deploys the elastic subchart of the vulnerability-assessment-tool-core chart v0.1.1
on the Kubernetes cluster in the default configuration. The configuration section lists
the parameters that can be configured during installation.

## .Uninstalling the chart
To uninstall/delete the `release` deployment:
```console
$ helm delete release
```

## .Configuration
The following table lists the configurable parameters of the elastic chart and their default values. As values in this chart are quite highly nested, this table is split into two parts for easy readability

### .Values.kibana
| Parameter | Description | Default |
|---|---|---|
| enabled | Kibana is an optional part of this chart and can be decoupled by setting this to `False` | `True` |
| replicas |  | `1` |
| debug | Sets Kibana logging level through environment variables. `LOGGING_VERBOSE` to True | `False` |
| serverHost |  | `0.0.0.0` |
| plugins.enabled | Installs a series of plugins | `False` |
| persistentVolume.enabled | Used to persist plugins and keep it from reoptimizing every time a pod gets launched | `False` |
| image.pullPolicy | Maintained by the elasticsearch team so quite stable | `IfNotPresent` |
| image.name |  | `kibana` |
| image.tag | image tag | `7.3.1` |
| livenessProbe |  | enabled: `True`<br>initialDelaySeconds: `30`<br>failureThreshold: `3`<br>periodSeconds: `10`<br>timeoutSeconds: `5` |
| readinessProbe |  | enabled: `True`<br>initialDelaySeconds: `15`<br>failureThreshold: `3`<br>periodSeconds: `10`<br>timeoutSeconds: `5` |


### .Values.elasticsearch
| Parameter | Description | Default |
| --- | --- | --- |
| debug | Creates a job called logSetterJob.yaml that sets the desired logging level of logger.org.elasticsearch.transport: if `True` then "trace" else "error" | `False` |
| clusterName | Name for the cluster (used by the headless service) | ` |
| nodeGroup |  | `master` |
| replicas | 3 is the minimum number of nodes in order to garantee fault tolerance | `3` |
| roles | Sets the role for each node | master: `true`<br>ingest: `true`<br>storage: `true` |
| esJavaOpts | Used in production to avoid java from exceeding the default heap limit | `-Xmx1g -Xms1g` |
| networkHost |  | `0.0.0.0` |
| sysctlVmMaxMapCount | Optimal system mmapfs count limit (see ![guide](https://www.elastic.co/guide/en/elasticsearch/reference/current/vm-max-map-count.html)) | `262144` |
| selfAntiAffinity | Weight for antiaffinity towards sharing a node with another elasticsearch node | `100` |
| updateStrategy |  | `RollingUpdate` |
| podManagementPolicy | Unordered management because ES is a self managed cluster | `Parallel` |
| terminationGracePeriodSeconds | Allows for elasticsearch cluster to gracefully end which speeds up election in the next deployment | `120` |
| clusterHealthCheckParams | query used for health chekc | `wait_for_status=green&timeout=1s` |
| podDisruptionBudget.maxUnavailable |  | `1` |
| image.initContainer.pullPolicy |  | `IfNotPresent` |
| image.initContainer.name |  | `busybox` |
| image.initContainer.tag |  | `1.31.0` |
| image.mainContainer.pullPolicy |  | `IfNotPresent` |
| image.mainContainer.name |  | `elasticsearch` |
| image.mainContainer.tag |  | `7.3.1` |
| image.mainContainer.readinessProbe | | failureThreshold: `3`<br>initialDelaySeconds: `10`<br>periodSeconds: `10`<br>successThreshold: `3`<br>timeoutSeconds: `5` |
| image.sidecarContainer.pullPolicy | | `IfNotPresent` |
| image.sidecarContainer.name |  | `elasticsearch` |
| image.sidecarContainer.tag | image tag | `7.3.1` |
| volumeClaimTemplate | PVC template for storing logs | accessModes:<br>&emsp; -`ReadWriteOnce`<br>&emsp;resources:<br>&emsp;&emsp;requests:<br>&emsp;&emsp;&emsp;storage: `40Gi` |

## Production configuration
This chart includes a `values_adv.yaml` file where you can find some parameters oriented to production configuration in comparison to the regular `values.yaml`.
```sh
$ helm install --name my-release -f values_adv.yaml .
```
These values can be configured as follows:

#These values should suffice for a normal deployment but not with plugins installation
### .Values.kibana
| Parameter | Description | Default |
| --- | --- | --- |
| selfAntiAffinity | Avoids sharing nodes with similar pods | soft: `True`<br>weight: `100` |
| elasticsearchAffinity | Stays close to ES pods | soft: `True`<br>weight: `100` |
| podDisruptionBudget.minAvailable | ensure availability during disruption | `1` |
| persistentVolume.storage | storage size for PVC | `1Gi` |
| securityContext |  | enabled: `True`<br>allowPrivilegeEscalation: `False`<br>runAsUser: `1000` |
| image.resources |  | requests:<br>&emsp;memory: `1G`<br>&emsp;cpu: `1000m`limit:<br>&emsp;memory: `2G`<br>&emsp;cpu: `2000m` |

### .Values.elasticsearch
| Parameter | Description | Default |
| --- | --- | --- |
| retention.max_size | Sets ILM max size | `100GB` |
| retention.max_age | Sets max age for logs | `180d` |
| retention.min_age | Sets min age for unused logs | `90d` |
| image.initContainer.resources |  | requests:<br>&emsp;memory: `128Mi`<br>&emsp;cpu: `25m`limit:<br>&emsp;memory: `150Mi`<br>&emsp;cpu: `25m` |
| image.mainContainer.resources |  | requests:<br>&emsp;memory: `1500Mi`<br>&emsp;cpu: `160m`limit:<br>&emsp;memory: `2Gi`<br>&emsp;cpu: `1000m` |
